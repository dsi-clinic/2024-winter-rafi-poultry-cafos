{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook prepares the training/testing masks and split file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Microsoft Corporation. All rights reserved\n",
    "# Licensed under the MIT License.\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import os\n",
    "import time\n",
    "import subprocess\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from multiprocessing import Pool\n",
    "import rasterio\n",
    "import fiona\n",
    "import shapely\n",
    "import shapely.geometry\n",
    "import shapely.ops\n",
    "\n",
    "from cafo import utils\n",
    "os.environ.update(utils.RASTERIO_BEST_PRACTICES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_ROOT_DIRECTORY = os.path.dirname(os.getcwd())\n",
    "BASE_OUTPUT_DIRECTORY = os.path.join(\n",
    "    REPO_ROOT_DIRECTORY,\n",
    "    \"output/masks/\"\n",
    ") # This is the base directory where the different mask datasets will be generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(\n",
    "    REPO_ROOT_DIRECTORY,\n",
    "    \"data/splits/\"\n",
    "), exist_ok=True)\n",
    "\n",
    "os.makedirs(BASE_OUTPUT_DIRECTORY, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_masks(naip_paths, polygon_fn, output_dir):\n",
    "    mask_fns = []\n",
    "    for i, naip_path in enumerate(naip_paths):\n",
    "        if i % 100 == 0:\n",
    "            print(i, len(naip_paths))\n",
    "        \n",
    "        mask_fn = os.path.join(output_dir, naip_path)\n",
    "        os.makedirs(os.path.dirname(mask_fn), exist_ok=True)\n",
    "        mask_fns.append(mask_fn)\n",
    "\n",
    "        naip_url = utils.NAIP_BLOB_ROOT + \"/\" + naip_path\n",
    "        with rasterio.open(naip_url) as f:\n",
    "            left, bottom, right, top = f.bounds\n",
    "            crs = f.crs.to_string()\n",
    "            height, width = f.height, f.width\n",
    "\n",
    "        command = [\n",
    "            \"gdal_rasterize\",\n",
    "            \"-ot\", \"Byte\",\n",
    "            \"-burn\", \"1\",\n",
    "            \"-of\", \"GTiff\",\n",
    "            \"-te\", str(left), str(bottom), str(right), str(top),\n",
    "            \"-ts\", str(width), str(height),\n",
    "            \"-co\", \"COMPRESS=LZW\",\n",
    "            \"-co\", \"BIGTIFF=YES\",\n",
    "            polygon_fn,\n",
    "            mask_fn\n",
    "        ]\n",
    "        subprocess.call(command)\n",
    "        \n",
    "    return mask_fns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all NAIP URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-all\n",
    "\n",
    "Uses NAIP imagery from every year for which we have data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygon_fn = \"../data/delmarva_training_set_polygons.geojson\"\n",
    "tiles_fn = \"../data/delmarva_training_set_tiles.geojson\"\n",
    "set_name = \"train-all/\"\n",
    "output_dir = os.path.join(BASE_OUTPUT_DIRECTORY, set_name)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "naip_paths = []\n",
    "with fiona.open(tiles_fn) as f:\n",
    "    for row in f:\n",
    "        naip_paths.append(row[\"properties\"][\"url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1983\n",
      "100 1983\n",
      "200 1983\n",
      "300 1983\n",
      "400 1983\n",
      "500 1983\n",
      "600 1983\n",
      "700 1983\n",
      "800 1983\n",
      "900 1983\n",
      "1000 1983\n",
      "1100 1983\n",
      "1200 1983\n",
      "1300 1983\n",
      "1400 1983\n",
      "1500 1983\n",
      "1600 1983\n",
      "1700 1983\n",
      "1800 1983\n",
      "1900 1983\n"
     ]
    }
   ],
   "source": [
    "# This will rasterize the shapes in \"polygon_fn\" to the extent of each imagery tile in \"naip_paths\" and\n",
    "# the resulting masks will be saved with the same directory structure / naming scheme under \"output_dir\"\n",
    "output_fns = generate_masks(naip_paths, polygon_fn, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict({\n",
    "    \"image_fn\": [utils.NAIP_BLOB_ROOT + \"/\" + naip_path for naip_path in naip_paths],\n",
    "    \"label_fn\": output_fns\n",
    "})\n",
    "df.to_csv(\"../data/splits/train-all.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-single\n",
    "\n",
    "Uses NAIP imagery from only VA 2016, MD 2017, and DE 2017 (the layers that the polygons were created with)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_naip_paths = []\n",
    "new_output_fns = []\n",
    "for i, fn in enumerate(output_fns):\n",
    "    if (\"de/2017/\" in fn) or (\"md/2017/\" in fn) or (\"va/2016/\" in fn): \n",
    "        new_output_fns.append(fn)\n",
    "        new_naip_paths.append(naip_paths[i])\n",
    "\n",
    "df = pd.DataFrame.from_dict({\n",
    "    \"image_fn\": [utils.NAIP_BLOB_ROOT + \"/\" + naip_path for naip_path in new_naip_paths],\n",
    "    \"label_fn\": new_output_fns\n",
    "})\n",
    "df.to_csv(\"../data/splits/train-single.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test-all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygon_fn = \"../data/delmarva_testing_set_polygons.geojson\"\n",
    "tiles_fn = \"../data/delmarva_testing_set_tiles.geojson\"\n",
    "set_name = \"test-all/\"\n",
    "output_dir = os.path.join(BASE_OUTPUT_DIRECTORY, set_name)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "naip_paths = []\n",
    "with fiona.open(tiles_fn) as f:\n",
    "    for row in f:\n",
    "        naip_paths.append(row[\"properties\"][\"url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 568\n",
      "100 568\n",
      "200 568\n",
      "300 568\n",
      "400 568\n",
      "500 568\n"
     ]
    }
   ],
   "source": [
    "output_fns = generate_masks(naip_paths, polygon_fn, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict({\n",
    "    \"image_fn\": [utils.NAIP_BLOB_ROOT + \"/\" + naip_path for naip_path in naip_paths],\n",
    "    \"label_fn\": output_fns\n",
    "})\n",
    "df.to_csv(\"../data/splits/test-all.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test-single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_naip_paths = []\n",
    "new_output_fns = []\n",
    "for i, fn in enumerate(output_fns):\n",
    "    if (\"de/2017/\" in fn) or (\"md/2017/\" in fn) or (\"va/2016/\" in fn): \n",
    "        new_output_fns.append(fn)\n",
    "        new_naip_paths.append(naip_paths[i])\n",
    "\n",
    "df = pd.DataFrame.from_dict({\n",
    "    \"image_fn\": [utils.NAIP_BLOB_ROOT + \"/\" + naip_path for naip_path in new_naip_paths],\n",
    "    \"label_fn\": new_output_fns\n",
    "})\n",
    "df.to_csv(\"../data/splits/test-single.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygon_fn = \"../data/Delmarva_PL_House_Final2_epsg26918.geojson\"\n",
    "tiles_fn = \"../data/delmarva_all_set_tiles.geojson\"\n",
    "set_name = \"all/\"\n",
    "output_dir = os.path.join(BASE_OUTPUT_DIRECTORY, set_name)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "naip_urls = []\n",
    "with fiona.open(tiles_fn) as f:\n",
    "    for row in f:\n",
    "        naip_urls.append(row[\"properties\"][\"url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 568\n",
      "100 568\n",
      "200 568\n",
      "300 568\n",
      "400 568\n",
      "500 568\n"
     ]
    }
   ],
   "source": [
    "output_fns = generate_masks(naip_paths, polygon_fn, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict({\n",
    "    \"image_fn\": [utils.NAIP_BLOB_ROOT + \"/\" + naip_path for naip_path in naip_paths],\n",
    "    \"label_fn\": output_fns\n",
    "})\n",
    "df.to_csv(\"../data/splits/all.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All-single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_naip_paths = []\n",
    "new_output_fns = []\n",
    "for i, fn in enumerate(output_fns):\n",
    "    if (\"de/2017/\" in fn) or (\"md/2017/\" in fn) or (\"va/2016/\" in fn): \n",
    "        new_output_fns.append(fn)\n",
    "        new_naip_paths.append(naip_paths[i])\n",
    "\n",
    "df = pd.DataFrame.from_dict({\n",
    "    \"image_fn\": [utils.NAIP_BLOB_ROOT + \"/\" + naip_path for naip_path in new_naip_paths],\n",
    "    \"label_fn\": new_output_fns\n",
    "})\n",
    "df.to_csv(\"../data/splits/all-single.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augment sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tile_shapes = []\n",
    "with fiona.open(\"../data/delmarva_testing_set_tiles.geojson\") as f:\n",
    "    for row in f:\n",
    "        test_tile_shapes.append(shapely.geometry.shape(row[\"geometry\"]))\n",
    "test_area = shapely.ops.cascaded_union(test_tile_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_shapes_into_train_test(fn):\n",
    "    shapes = []\n",
    "    geoms = []\n",
    "    with fiona.open(fn) as f:\n",
    "        for row in f:\n",
    "            shapes.append(shapely.geometry.shape(row[\"geometry\"]))\n",
    "            geoms.append(row)\n",
    "            \n",
    "    test_geoms = []\n",
    "    train_geoms = []\n",
    "    for i, shape in enumerate(shapes):\n",
    "        if test_area.intersects(shape):\n",
    "            test_geoms.append(geoms[i])\n",
    "        else:\n",
    "            train_geoms.append(geoms[i])\n",
    "    \n",
    "    return train_geoms, test_geoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fns = [\n",
    "    \"../data/poultry_barn_change_predictions/poultry_barns-64-200_predictions_2011.geojson\",\n",
    "    \"../data/poultry_barn_change_predictions/poultry_barns-64-200_predictions_2012.geojson\",\n",
    "    \"../data/poultry_barn_change_predictions/poultry_barns-64-200_predictions_2013.geojson\",\n",
    "    \"../data/poultry_barn_change_predictions/poultry_barns-64-200_predictions_2014.geojson\",\n",
    "    \"../data/poultry_barn_change_predictions/poultry_barns-64-200_predictions_2015.geojson\",\n",
    "    \"../data/poultry_barn_change_predictions/poultry_barns-64-200_predictions_2016.geojson\",\n",
    "    \"../data/poultry_barn_change_predictions/poultry_barns-64-200_predictions_2017.geojson\",\n",
    "    \"../data/poultry_barn_change_predictions/poultry_barns-64-200_predictions_2018.geojson\",\n",
    "]\n",
    "\n",
    "for fn in fns:\n",
    "    \n",
    "    train_fn = fn.replace(\".geojson\", \"_train.geojson\")\n",
    "    test_fn = fn.replace(\".geojson\", \"_test.geojson\")\n",
    "    \n",
    "    train_geoms, test_geoms = split_shapes_into_train_test(fn)\n",
    "    \n",
    "    with fiona.open(fn) as f:\n",
    "        schema = f.schema.copy()\n",
    "        crs = f.crs.copy()\n",
    "        \n",
    "    with fiona.open(train_fn, \"w\", driver=\"GeoJSON\", schema=schema, crs=crs) as f:\n",
    "        f.writerecords(train_geoms)\n",
    "        \n",
    "    with fiona.open(test_fn, \"w\", driver=\"GeoJSON\", schema=schema, crs=crs) as f:\n",
    "        f.writerecords(test_geoms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles_fn = \"../data/delmarva_training_set_tiles.geojson\"\n",
    "set_name = \"train-augment/\"\n",
    "output_dir = os.path.join(BASE_OUTPUT_DIRECTORY, set_name)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "polygon_fns_by_year = {\n",
    "    year: \"../data/poultry_barns-64-200_predictions_%d_train.geojson\" % year\n",
    "    for year in range(2011,2019)\n",
    "}\n",
    "\n",
    "naip_urls = []\n",
    "with fiona.open(tiles_fn) as f:\n",
    "    for row in f:\n",
    "        naip_urls.append(row[\"properties\"][\"url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 568\n",
      "100 568\n",
      "200 568\n",
      "300 568\n",
      "400 568\n",
      "500 568\n"
     ]
    }
   ],
   "source": [
    "output_fns = []\n",
    "for i, naip_path in enumerate(naip_paths):\n",
    "    if i % 100 == 0:\n",
    "        print(i, len(naip_paths))\n",
    "\n",
    "    year = int(naip_path.split(\"/\")[2])\n",
    "    polygon_fn = polygon_fns_by_year[year]\n",
    "        \n",
    "    mask_fn = os.path.join(output_dir, naip_path)\n",
    "    os.makedirs(os.path.dirname(mask_fn), exist_ok=True)\n",
    "    output_fns.append(mask_fn)\n",
    "\n",
    "    naip_url = utils.NAIP_BLOB_ROOT + \"/\" + naip_path\n",
    "    with rasterio.open(naip_url) as f:\n",
    "        left, bottom, right, top = f.bounds\n",
    "        crs = f.crs.to_string()\n",
    "        height, width = f.height, f.width\n",
    "\n",
    "    command = [\n",
    "        \"gdal_rasterize\",\n",
    "        \"-ot\", \"Byte\",\n",
    "        \"-burn\", \"1\",\n",
    "        \"-of\", \"GTiff\",\n",
    "        \"-te\", str(left), str(bottom), str(right), str(top),\n",
    "        \"-ts\", str(width), str(height),\n",
    "        \"-co\", \"COMPRESS=LZW\",\n",
    "        \"-co\", \"BIGTIFF=YES\",\n",
    "        polygon_fn,\n",
    "        mask_fn\n",
    "    ]\n",
    "    subprocess.call(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict({\n",
    "    \"image_fn\": [utils.NAIP_BLOB_ROOT + \"/\" + naip_path for naip_path in naip_paths],\n",
    "    \"label_fn\": output_fns\n",
    "})\n",
    "df.to_csv(\"../data/splits/train-augment.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cafo",
   "language": "python",
   "name": "conda-env-cafo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
